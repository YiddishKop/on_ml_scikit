#+TITLE: Feature Selection


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

#+END_SRC

* Automatic Feature Selection
Often we collected many features that might be related to a supervised
prediction task, but we don't know which of them are actually predictive. To
improve interpretability, and sometimes also generalization performance, we can
use automatic feature selection to select a subset of the original features.
There are several types of feature selection methods available, which we'll
explain in order of increasing complexity.

For a given supervised model, the best feature selection strategy would be to
try out each possible subset of the features, and evaluate generalization
performance using this subset. However, there are exponentially many subsets of
features, so this exhaustive search is generally infeasible. The strategies
discussed below can be thought of as proxies for this infeasible computation.

** Univariate statistics
The simplest method to select features is using univariate statistics, that is
by looking at each feature individually and running a statistical test to see
whether it is related to the target. This kind of test is also known as analysis
of variance (ANOVA).

We create a synthetic dataset that consists of the breast cancer data with an
additional 50 completely random features.


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.datasets import load_breast_cancer, load_digits
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
# get deterministic random numbers
rng = np.random.RandomState(42)
noise = rng.normal(size=(len(cancer.data), 50))
# add noise features to the data
# the first 30 features are from the dataset, the next 50 are noise
X_w_noise = np.hstack([cancer.data, noise])
X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target,
                                                    random_state=0, test_size=.5)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[193]:
:END:

We have to define a threshold on the p-value of the statistical test to decide
how many features to keep. There are several strategies implemented in
scikit-learn, a straight-forward one being SelectPercentile, which selects a
percentile of the original features (we select 50% below):


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.feature_selection import SelectPercentile
# use f_classif (the default) and SelectPercentile to select 50% of features:
select = SelectPercentile(percentile=50)
select.fit(X_train, y_train)
# transform training set:
X_train_selected = select.transform(X_train)
print(X_train.shape)
print(X_train_selected.shape)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[194]:
:END:

We can also use the test statistic directly to see how relevant each feature is.
As the breast cancer dataset is a classification task, we use f_classif, the
F-test for classification. Below we plot the p-values associated with each of
the 80 features (30 original features + 50 noise features). Low p-values
indicate informative features.


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
  from sklearn.feature_selection import f_classif, f_regression, chi2

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[195]:
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
  F, p = f_classif(X_train, y_train)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[196]:
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
  plt.figure()
  plt.plot(p, 'o')

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[197]:
: [<matplotlib.lines.Line2D at 0x7f1ae8e1b8d0>]
[[file:./obipy-resources/8573gOg.png]]
:END:

Clearly most of the first 30 features have very small p-values.

Going back to the SelectPercentile transformer, we can obtain the features that
are selected using the get_support method:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
mask = select.get_support()
print(mask)
# visualize the mask. black is True, white is False
plt.matshow(mask.reshape(1, -1), cmap='gray_r')

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[198]:
: <matplotlib.image.AxesImage at 0x7f1ae8bb6828>
[[file:./obipy-resources/8573tYm.png]]
:END:

Nearly all of the original 30 features were recovered. We can also analize the
utility of the feature selection by training a supervised model on the data.
It's important to learn the feature selection only on the training set!


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.linear_model import LogisticRegression
# transform test data:
X_test_selected = select.transform(X_test)
lr = LogisticRegression()
lr.fit(X_train, y_train)
print("Score with all features: %f" % lr.score(X_test, y_test))
lr.fit(X_train_selected, y_train)
print("Score with only selected features: %f" % lr.score(X_test_selected, y_test))

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[199]:
:END:

** Model-based Feature Selection
A somewhat more sophisticated method for feature selection is using a supervised
machine learning model and selecting features based on how important they were
deemed by the model. This requires the model to provide some way to rank the
features by importance. This can be done for all tree-based models (which
implement get_feature_importances) and all linear models, for which the
coefficients can be used to determine how much influence a feature has on the
outcome.

Any of these models can be made into a transformer that does feature selection
by wrapping it with the SelectFromModel class:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold="median")

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[200]:
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
select.fit(X_train, y_train)
X_train_rf = select.transform(X_train)
print(X_train.shape)
print(X_train_rf.shape)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[201]:
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
mask = select.get_support()
# visualize the mask. black is True, white is False
plt.matshow(mask.reshape(1, -1), cmap='gray_r')

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[202]:
: <matplotlib.image.AxesImage at 0x7f1ae8d2a588>
[[file:./obipy-resources/85736is.png]]
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
X_test_rf = select.transform(X_test)
LogisticRegression().fit(X_train_rf, y_train).score(X_test_rf, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[203]:
: 0.9508771929824561
:END:

This method builds a single model (in this case a random forest) and uses the
feature importances from this model. We can do a somewhat more elaborate search
by training multiple models on subsets of the data. One particular strategy is
recursive feature elimination:

** Recursive Feature Elimination
Recursive feature elimination builds a model on the full set of features, and
similar to the method above selects a subset of features that are deemed most
important by the model. However, usually only a single feature is dropped from
the dataset, and a new model is built with the remaining features. The process
of dropping features and model building is repeated until there are only a
pre-specified number of features left:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.feature_selection import RFE
select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)
select.fit(X_train, y_train)
# visualize the selected features:
mask = select.get_support()
plt.matshow(mask.reshape(1, -1), cmap='gray_r')

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[204]:
: <matplotlib.image.AxesImage at 0x7f1ae8c36828>
[[file:./obipy-resources/8573Hty.png]]
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
X_train_rfe = select.transform(X_train)
X_test_rfe = select.transform(X_test)
LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[205]:
: 0.9508771929824561
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
select.score(X_test, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[206]:
: 0.9508771929824561
:END:

EXERCISE: Create the "XOR" dataset as in the first cell below: Add random
features to it and compare how univariate selection compares to model based
selection using a Random Forest in recovering the original features.

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
import numpy as np
rng = np.random.RandomState(1)
# Generate 400 random integers in the range [0, 1]
X = rng.randint(0, 2, (200, 2))
y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)  # XOR creation
plt.scatter(X[:, 0], X[:, 1], c=plt.cm.spectral(y.astype(float)))

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[207]:
: <matplotlib.collections.PathCollection at 0x7f1ae9129cc0>
[[file:./obipy-resources/857352B.png]]
:END:

# %load solutions/19_univariate_vs_mb_selection.py
