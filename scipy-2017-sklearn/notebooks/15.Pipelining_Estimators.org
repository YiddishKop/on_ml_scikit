#+TITLE: Pipeline Estimators

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[137]:
:END:

* Pipelining estimators
In this section we study how different estimators maybe be chained.

** A simple example: feature extraction and selection before an estimator
*** Feature extraction: vectorizer
For some types of data, for instance text data, a feature extraction step must
be applied to convert it to numerical features. To illustrate we load the SMS
spam dataset we used earlier.


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
import os
with open(os.path.join("datasets", "smsspam", "SMSSpamCollection")) as f:
    lines = [line.strip().split("\t") for line in f.readlines()]
text = [x[1] for x in lines]
y = [x[0] == "ham" for x in lines]

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[138]:
:END:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.model_selection import train_test_split
text_train, text_test, y_train, y_test = train_test_split(text, y)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[139]:
:END:

Previously, we applied the feature extraction manually, like so:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
vectorizer = TfidfVectorizer()
vectorizer.fit(text_train)
X_train = vectorizer.transform(text_train)
X_test = vectorizer.transform(text_test)
clf = LogisticRegression()
clf.fit(X_train, y_train)
clf.score(X_test, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[140]:
: 0.9784791965566715
:END:

The situation where we learn a transformation and then apply it to the test data
is very common in machine learning. Therefore scikit-learn has a shortcut for
this, called pipelines:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.pipeline import make_pipeline
pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())
pipeline.fit(text_train, y_train)
pipeline.score(text_test, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[141]:
: 0.9784791965566715
:END:

As you can see, this makes the code much shorter and easier to handle. Behind
the scenes, exactly the same as above is happening. When calling fit on the
pipeline, it will call fit on each step in turn. ​ After the first step is fit,
it will use the ``transform`` method of the first step to create a new
representation. This will then be fed to the ``fit`` of the next step, and so
on. Finally, on the last step, only ``fit`` is called. ​

file:figures/pipeline.png

If we call ``score``, only ``transform`` will
be called on each step - this could be the test set after all! Then, on the last
step, ``score`` is called with the new representation. The same goes for
``predict``.

Building pipelines not only simplifies the code, it is also important for model
selection. Say we want to grid-search C to tune our Logistic Regression above.

Let's say we do it like this:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
# This illustrates a common mistake. Don't use this code!
from sklearn.model_selection import GridSearchCV
vectorizer = TfidfVectorizer()
vectorizer.fit(text_train)
X_train = vectorizer.transform(text_train)
X_test = vectorizer.transform(text_test)
clf = LogisticRegression()
grid = GridSearchCV(clf, param_grid={'C': [.1, 1, 10, 100]}, cv=5)
grid.fit(X_train, y_train)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[142]:
#+BEGIN_EXAMPLE
  GridSearchCV(cv=5, error_score='raise',
  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
  penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
  verbose=0, warm_start=False),
  fit_params=None, iid=True, n_jobs=1,
  param_grid={'C': [0.1, 1, 10, 100]}, pre_dispatch='2*n_jobs',
  refit=True, return_train_score='warn', scoring=None, verbose=0)
#+END_EXAMPLE
:END:

*** What did we do wrong?

Here, we did grid-search with cross-validation on X_train. However, when
applying TfidfVectorizer, it saw all of the X_train, not only the training
folds! So it could use knowledge of the frequency of the words in the
test-folds. This is called "contamination" of the test set, and leads to too
optimistic estimates of generalization performance, or badly selected
parameters. We can fix this with the pipeline, though:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.model_selection import GridSearchCV
pipeline = make_pipeline(TfidfVectorizer(),
                         LogisticRegression())
grid = GridSearchCV(pipeline,
                    param_grid={'logisticregression__C': [.1, 1, 10, 100]}, cv=5)
grid.fit(text_train, y_train)
grid.score(text_test, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[143]:
: 0.9892395982783357
:END:

Note that we need to tell the pipeline where at which step we wanted to set the
parameter C. We can do this using the special __ syntax. The name before the __
is simply the name of the class, the part after __ is the parameter we want to
set with grid-search.


file:figures/pipeline_cross_validation.png

Another benefit of using pipelines is that we can now also search over
parameters of the feature extraction with GridSearchCV:


#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.model_selection import GridSearchCV
pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())
params = {'logisticregression__C': [.1, 1, 10, 100],
          "tfidfvectorizer__ngram_range": [(1, 1), (1, 2), (2, 2)]}
grid = GridSearchCV(pipeline, param_grid=params, cv=5)
grid.fit(text_train, y_train)
print(grid.best_params_)
grid.score(text_test, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[144]:
: 0.9892395982783357
:END:

EXERCISE: Create a pipeline out of a StandardScaler and Ridge regression and
apply it to the Boston housing dataset (load using
sklearn.datasets.load_boston). Try adding the
sklearn.preprocessing.PolynomialFeatures transformer as a second preprocessing
step, and grid-search the degree of the polynomials (try 1, 2 and 3).

# %load solutions/15A_ridge_grid.py
