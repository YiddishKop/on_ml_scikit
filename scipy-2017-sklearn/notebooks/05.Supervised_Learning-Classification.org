#+TITLE: Supervised Learning Part1 -- Classification

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[103]:
:END:

* Supervised Learning Part 1 -- Classification

To visualize the workings of machine learning algorithms, it is often helpful to
study two-dimensional or one-dimensional data, that is data with only one or two
features. While in practice, datasets usually have many more features, it is
hard to plot high-dimensional data in on two-dimensional screens.

We will illustrate some very simple examples before we move on to more "real
world" data sets.

First, we will look at a two class classification problem in two dimensions. We
use the synthetic data generated by the make_blobs function.

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.datasets import make_blobs

X, y = make_blobs(centers=2, random_state=0)

print('X ~ n_samples x n_features:', X.shape)
print('y ~ n_samples:', y.shape)

print('\nFirst 5 samples:\n', X[:5, :])
print('\nFirst 5 labels:', y[:5])

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[104]:
:END:

As the data is two-dimensional, we can plot each sample as a point in a
two-dimensional coordinate system, with the first feature being the x-axis and
the second feature being the y-axis.

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
plt.scatter(X[y == 0, 0], X[y == 0, 1],
            c='blue', s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1],
            c='red', s=40, label='1', marker='s')

plt.xlabel('first feature')
plt.ylabel('second feature')
plt.legend(loc='upper right');

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[105]:
[[file:./obipy-resources/1942ojP.png]]
:END:

Classification is a supervised task, and since we are interested in its
performance on unseen data, we split our data into two parts:

- a training set that the learning algorithm uses to fit the model

- a test set to evaluate the generalization performance of the model

The train_test_split function from the model_selection module does that for us
-- we will use it to split a dataset into 75% training data and 25% test data.

[[file:figures/train_test_split_matrix.png]]



#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.25,
                                                    random_state=1234,
                                                    stratify=y)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[106]:
:END:

** The scikit-learn estimator API

https://prnt.sc/jr49ti

Every algorithm is exposed in scikit-learn via an ''Estimator'' object. (All
models in scikit-learn have a very consistent interface). For instance, we first
import the logistic regression class.

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.linear_model import LogisticRegression
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[107]:
:END:

Next, we instantiate the estimator object.

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
classifier = LogisticRegression()
X_train.shape
y_train.shape
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[108]:
: (75,)
:END:

To built the model from our data, that is to learn how to classify new points,
we call the fit function with the training data, and the corresponding training
labels (the desired output for the training data point):

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
classifier.fit(X_train, y_train)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[109]:
#+BEGIN_EXAMPLE
  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
  penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
  verbose=0, warm_start=False)
#+END_EXAMPLE
:END:

(Some estimator methods such as fit return self by default. Thus, after
executing the code snippet above, you will see the default parameters of this
particular instance of LogisticRegression. Another way of retrieving the
estimator's ininitialization parameters is to execute classifier.get_params(),
which returns a parameter dictionary.)

We can then apply the model to unseen data and use the model to predict the
estimated outcome using the predict method:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
prediction = classifier.predict(X_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[110]:
:END:

We can compare these against the true labels:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
print(prediction)
print(y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[111]:
:END:

We can evaluate our classifier quantitatively by measuring what fraction of
predictions is correct. This is called accuracy:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
np.mean(prediction == y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[112]:
: 0.83999999999999997
:END:

There is also a convenience function , score, that all scikit-learn classifiers
have to compute this directly from the test data:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
classifier.score(X_test, y_test)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[113]:
: 0.83999999999999997
:END:

It is often helpful to compare the generalization performance (on the test set)
to the performance on the training set:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
classifier.score(X_train, y_train)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[114]:
: 0.94666666666666666
:END:

LogisticRegression is a so-called linear model, that means it will create a
decision that is linear in the input space. In 2d, this simply means it finds a
line to separate the blue from the red:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from figures import plot_2d_separator

plt.scatter(X[y == 0, 0], X[y == 0, 1],
            c='blue', s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1],
            c='red', s=40, label='1', marker='s')

plt.xlabel("first feature")
plt.ylabel("second feature")
plot_2d_separator(classifier, X)
plt.legend(loc='upper right');

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[115]:
[[file:./obipy-resources/19421tV.png]]
:END:

Estimated parameters: All the estimated model parameters are attributes of the
estimator object ending by an underscore. Here, these are the coefficients and
the offset of the line:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
print(classifier.coef_)
print(classifier.intercept_)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[116]:
:END:

** Another classifier: K Nearest Neighbors
Another popular and easy to understand classifier is K nearest neighbors (kNN).
It has one of the simplest learning strategies: given a new, unknown
observation, look up in your reference database which ones have the closest
features and assign the predominant class.

The interface is exactly the same as for LogisticRegression above.

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
from sklearn.neighbors import KNeighborsClassifier

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[117]:
:END:

This time we set a parameter of the KNeighborsClassifier to tell it we only want
to look at one nearest neighbor:

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
knn = KNeighborsClassifier(n_neighbors=1)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[118]:
:END:

We fit the model with out training data

#+BEGIN_SRC ipython :session :exports both :async t :results raw drawer
knn.fit(X_train, y_train)
plt.scatter(X[y == 0, 0], X[y == 0, 1],
            c='blue', s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1],
            c='red', s=40, label='1', marker='s')

plt.xlabel("first feature")
plt.ylabel("second feature")
plot_2d_separator(knn, X)
plt.legend(loc='upper right');
knn.score(X_test, y_test)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[119]:
: 1.0
[[file:./obipy-resources/1942C4b.png]]
:END:

EXERCISE: Apply the KNeighborsClassifier to the ``iris`` dataset. Play with
different values of the ``n_neighbors`` and observe how training and test score
change.
# %load solutions/05A_knn_with_diff_k.py
